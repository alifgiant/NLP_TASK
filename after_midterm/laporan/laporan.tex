%%
% Written by: Muhammad Alif Akbar, Informatics Engineering, Telkom University.
% December, 2016
%%

\documentclass[conference,compsoc]{IEEEtran}
% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** CITATION PACKAGES ***
%
\ifCLASSOPTIONcompsoc
  % IEEE Computer Society needs nocompress option
  % requires cite.sty v4.0 or later (November 2003)
  \usepackage[nocompress]{cite}
\else
  % normal IEEE
  \usepackage{cite}
\fi
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.
%
% Note that some packages require special options to format as the Computer
% Society requires. In particular, Computer Society  papers do not use
% compressed citation ranges as is done in typical IEEE papers
% (e.g., [1]-[4]). Instead, they list every citation separately in order
% (e.g., [1], [2], [3], [4]). To get the latter we need to load the cite
% package with the nocompress option which is supported by cite.sty v4.0
% and later.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
% \interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.

\usepackage{tabularx,ragged2e,booktabs,caption}
% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=footnotesize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a sans serif font rather
% than the serif font used in traditional IEEE formatting and thus the need
% to invoke different subfig.sty package options depending on whether
% compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF, URL AND HYPERLINK PACKAGES ***
%
\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.


% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{HMM POS Tagging of Twitter's Tweet}


% conference papers do not typically use \thanks and this command
% is locked out in conference mode. If really needed, such as for
% the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% after \documentclass

% for over three affiliations, or if they all won't fit within the width
% of the page (and note that there is less available width in this regard for
% compsoc conferences compared to traditional conferences), use this
% alternative format:
% 
\author{\IEEEauthorblockN{Muhammad Alif Akbar}
\IEEEauthorblockA{School of Computing, Informatics Departement\\
Telkom University}}


% use for special paper notices
%\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle


% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Twitter has become a main stream channel of communication. With a lot of conversation took place on it, Twitter can be a source to learn many things. The project build an HMM classifier to map a words in a sentence (tweet) on twitter into their proper tags. The HMM is build using unigram, bigram, and trigram model that calculate normalized word-tag. The method has proven effective with accuracy up to 94\% for the task.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
Pos tagging, twitter, HMM, NLP.
\end{IEEEkeywords}



% For peer review papers, you can put extra information on the cover
% page as needed:
% \ifCLASSOPTIONpeerreview
% \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% \fi
%
% For peerreview papers, this IEEEtran command inserts a page break and
% creates the second title. It will be ignored for other modes.
\IEEEpeerreviewmaketitle



\section{Introduction}
% no \IEEEPARstart
Twitter has become a main stream channel of communication. With a lot of conversation took place on it, Twitter can be a source to learn many things. But, the conversation is just too big to be learned by a human so machine learning solution should be built. Natural Language Processing (NLP) has been an interesting research recently\cite{POS:1,POS:2}. One step on NLP is POS Tagging. Even the POS tagging is easy on well-structured sentences. It can be a challenge on unstructured sentence like be found on twitter. A sentence can consist of URL, Hashtag, Username, Symbol(emoticon) and Re-Tweet also slang words such as "LoL", "Hahaha", etc. So our project tried to do automatic tagging because with a good tagging every sentence can be learned further such as information retrieval (IR), sentiment analysis, etc.

\section{Proposed Method}
This project proposed to build classifier based on HMM to classify words into their tags. The project use tags that defined by Penn Tree Bank (PTB) that consist of 45 tags \cite{POStag:PTB}. But those tags are not enough to map tweet sentences, so there are added 4 more tags that are URL, USR, HT, and RT. Then the data are passed into pre-processing, processing and testing.
\subsection{Twitter}
Twitter is one on notable website of social network. Twitter founded in 2006 by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams\cite{wiki:Twitter}. By 2013 Twitter generated 340 million tweet per day\cite{blog:Twitter}. A sentence (tweet) in twitter can only consist of 140 characters that also containt url(regex: http://[.]+\textbackslash .[.]+), hashtags (regex: \#[.]+), symbols and username (regex: @[.]+).
\subsection{Dataset}
Dataset are obtained from GATE Twitter part-of-speech tagger \cite{GateTAGE}. It consist of 159492 sentences with words are already paired by their POS tag. The dataset then split by 70-30 into training and testing dataset, 111644 and 47848 respectivly. This done to simulate unknown data to the system. The tags that already on dataset is defined by Penn Tree Bank (PTB) P.O.S Tags and added 4 Tags to define twitter word class, hashtag, username, re-tweet, and url link with total 49 tags. The total appearance of tags is 1543040 as can be seen on table 1 and table 2.

\subsection{HMM}
Data processed using a Hidden Markov Model (HMM) classifier with the viterbi algorithm. HMM is chosen because it's simplicity and high accuracy for P.o.S Tagging\cite{HMM:1,HMM:2}. HMM is a classifier belong to statistical/bayesian family based on transition and emission probability. The HMM using will based on combination probability using unigram, bigram, and trigram word model. The HMM model is formulated to
\[ P(w_1..n|tag_1..n) = \prod_{i=1}^n emission(w_i|t_i) \prod_{i=1}^n transition(t_i|t_{i-1}) \]
Emision probability calculated using formula:
\[ e = \frac{count(word_i, tag_i)}{count(tag_i)} \]
Transition probability calculated using formula based on unigram of the tags:
\[ t = \frac{count(tag_i)}{\sum_i^n count(tag_i)} \]
Transition probability calculated using formula based on bigram of the tags:
\[ t = \frac{count(tag_i,tag_{i-1})}{count(tag_{i-1})} \]
Transition probability calculated using formula based on trigram of the tags:
\[ t = \frac{count(tag_i,tag_{i-1},tag_{i-2})}{count(tag_{i-1},tag_{i-2})} \]


\begin{center}
	\captionof{table}{Tags Appearance on Training}
	\begin{tabular}{|l|c|c|}
		\hline
			\bf Tag Definition & \bf Tag Code & \bf Appearance \\
		\hline
			Dollar & \$ & 10 \\
			Opening quotation mark & `` & 642 \\
			Closing quotation mark & '' & 1162 \\
			Opening parenthesis & ( & 736 \\
			Closing parenthesis & ) & 1699 \\
			Comma & , & 15897 \\
			Dash & -- & 0 \\
			Sentence terminator & . & 72858 \\
			Colon or ellipsis & : & 43282 \\
			Conjunction, coordinating & CC & 15790 \\
			Numeral, cardinal & CD & 8917 \\
			Determiner & DT & 52546 \\
			Existential there & EX & 270 \\
			Foreign word & FW & 27 \\
			Twitter Hash Tag & HT & 29030 \\
			Prep or conj, subor & IN & 63300 \\
			Adjective or numeral, ordinal & JJ & 44422 \\
			Adjective, comparative & JJR & 1742 \\
			Adjective, superlative & JJS & 2184 \\
			List item marker & LS & 0 \\
			Modal auxiliary & MD & 14952 \\
			Noun, singular or mass & NN & 106256 \\
			Noun, proper, singular & NNP & 31274 \\
			Noun, proper, plural & NNPS & 66 \\
			Noun, common, plural & NNS & 27861 \\
			Pre-determiner & PDT & 3 \\
			Genitive marker & POS & 1549 \\
			Pronoun, personal & PRP & 102569 \\
			Pronoun, possessive & PRP\$ & 20311 \\
			Adverb & RB & 55515 \\
			Adverb, comparative & RBR & 330 \\
			Adverb, superlative & RBS & 193 \\
			Particle & RP & 4449 \\
			Twitter re-tweet & RT & 28327 \\
			Symbol & SYM & 24 \\
			To & TO & 20219 \\
			Interjection & UH & 36624 \\
			Twitter URL & URL & 12956 \\
			Twitter username & USR & 71891 \\
			Verb, base form & VB & 53364 \\
			Verb, past tense &VBD & 19524 \\
			Verb, present participle & VBG & 19864 \\
			Verb, past participle & VBN & 7801 \\
			verb, present, singular & VBP & 54885 \\
			verb, present, 3rd person & VBZ & 19929 \\
			WH-determiner & WDT & 51 \\
			WH-pronoun & WP & 5658 \\
			WH-pronoun, possessive & WP\$ & 0 \\
			Wh-adverb & WRB & 8574 \\
		\hline	
	\end{tabular} 
\end{center}
\bigskip
\bigskip
\begin{center}
	\captionof{table}{Tags Appearance on Testing}
	\begin{tabular}{|l|c|c|}
		\hline
			\bf Tag Definition & \bf Tag Code & \bf Appearance \\
		\hline
			Dollar & \$ & 2 \\
			Opening quotation mark & `` & 255 \\
			Closing quotation mark & '' & 450 \\
			Opening parenthesis & ( & 291 \\
			Closing parenthesis & ) & 730 \\
			Comma & , & 6660 \\
			Dash & -- & 0 \\
			Sentence terminator & . & 31026 \\
			Colon or ellipsis & : & 18576 \\
			Conjunction, coordinating & CC & 6809 \\
			Numeral, cardinal & CD & 3459 \\
			Determiner & DT & 22515 \\
			Existential there & EX & 107 \\
			Foreign word & FW & 6 \\
			Twitter Hash Tag & HT & 12245 \\
			Prep or conj, subor & IN & 26943 \\
			Adjective or numeral, ordinal & JJ & 18873 \\
			Adjective, comparative & JJR & 666 \\
			Adjective, superlative & JJS & 872 \\
			List item marker & LS & 0 \\
			Modal auxiliary & MD & 6661 \\
			Noun, singular or mass & NN & 45864 \\
			Noun, proper, singular & NNP & 13256 \\
			Noun, proper, plural & NNPS & 30 \\
			Noun, common, plural & NNS & 11734 \\
			Pre-determiner & PDT & 0 \\
			Genitive marker & POS & 631 \\
			Pronoun, personal & PRP & 144906 \\
			Pronoun, possessive & PRP\$ & 8834 \\
			Adverb & RB & 24277 \\
			Adverb, comparative & RBR & 134 \\
			Adverb, superlative & RBS & 125 \\
			Particle & RP & 1997 \\
			Twitter re-tweet & RT & 12076 \\
			Symbol & SYM & 5 \\
			To & TO & 8703 \\
			Interjection & UH & 15621 \\
			Twitter URL & URL & 5283 \\
			Twitter username & USR & 30858 \\
			Verb, base form & VB & 23053 \\
			Verb, past tense &VBD & 8354 \\
			Verb, present participle & VBG & 8491 \\
			Verb, past participle & VBN & 3288 \\
			verb, present, singular & VBP & 24013 \\
			verb, present, 3rd person & VBZ & 8518 \\
			WH-determiner & WDT & 31 \\
			WH-pronoun & WP & 2494 \\
			WH-pronoun, possessive & WP\$ & 0 \\
			Wh-adverb & WRB & 3687 \\
		\hline	
	\end{tabular} 
\end{center}

as can be seen on table 1 and 2, there are tags of PTB that never appear neither on training sets nor testing sets. This will become a problem later on calculating joint probabilities.

\bigskip

\section{Experiment and Testing}
\subsection{Preprocessing}
Using HMM means the project need to conditional probability tables (CPT\emph{s}). Which mean each word and tags are need to be counted and calculate their appearance probability. There is a problem in building cpt\emph{s} that is counting word. The problem in counting word is that some words are uncommon in the set (appear very little). This can be due to they are slang words, mistyped, shortened, person name, numerical, etc. This can lead to very big word set leading into very long algorithm execution time. 

So to handle the problem the project normalize low-frequent words. In training process, words are counted as it is, then the table count is re-iterate to look for apperance count below 5 ($count(word_i)<5$) to normalize them and set them a new word-class. In testing process, words are checked if exist on the cpt then get its probability or else if not exist the words are also normalized. The word-class are defined by Bicke et. (1999), can be seen on table 3:
\begin{center}
	\captionof{table}{Low-Freq World Class}
	\begin{tabular}{|l|l|}
		\hline
			\bf Word class & \bf Intuition\\
		\hline
		twoDigitNum & two digit year \\
	    FourDigitNum & four digit year \\
	    ContainsDigitAndAlpha & product code \\
	    ContainsDigitAndDash & date, dash \\
	    ContainsDigitAndSlash & date, slash \\
	    ContainsDigitAndComma & monetary amount \\
	    ContainsDigitAndPeriod & monetary amount, percentage \\
	    OtherNum & other number \\
	    AllCaps & organization \\
	    CapPeriod & personal name initial \\
	    FirstWord & first word capital, sentence \\
	    InitCap & first word capital \\
	    Lowercase & uncapitalized \\
	    Other & other, symbol \\
	    OtherCap & other capital \\
		\hline	
	\end{tabular} 
\end{center}

\subsection{Processing}
The project build using HMM that put combination of unigram, bigram, and trigram word model into probability calculation. With each probabilty muliplied by a constant $\lambda_i$, with $\sum \lambda_i = 1$. Combination of unigram, bigram, and trigram hoped to increase the accuracy of system.
\[ transition = \lambda_1 \times t_{unigram} + \lambda_2 \times t_{bigram} + \lambda_3 \times t_{trigram} \]

'Zero occurance' problem is another problem in building cpt. When an unknown word or tags put into the classifier system. 'Zero occurance' can lead into classifier error or zero result calculation. Which is bad because can reduce system accuracy. As been seen on the table 1 and 2 there are some tag that never been seen in training nor testing set. So to cancel zero occurance every tags appearance is added by 1 (+1) so that every tags are considered to appear at least 1. Also the project do not use any outside corpus so smoothing also done to every word-tag pair, (added by 1) which mean every words considered is paired with any tag at least 1 time. This smoothing lead a bit change to both emission and transision formul
Emision probability calculated using formula:
\[ e = \frac{count(word_i, tag_i)+1}{count(tag_i)+\sum_{i=1}^n 1} \]
Transition probability calculated using formula for unigram matrix of the tags:
\[ t_{unigram} = \frac{count(tag_i)+1}{\sum_i^n count(tag_i)+\sum_{i=1}^n 1} \]
Transition probability calculated using formula for unigram matrix of the tags:
\[ t_{bigram} = \frac{count(tag_i,tag_{i-1})+1}{count(tag_{i-1})+\sum_{i=1}^{n^2} 1} \]
Transition probability calculated using formula for trigram matrix of the tags:
\[ t_{trigram} = \frac{count(tag_i,tag_{i-1},tag_{i-2})+1}{count(tag_{i-1},tag_{i-2})+\sum_{i=1}^{n^3} 1} \]
'zero occurance' must be eliminated because whenever a word or tag chain on testing set never appear on training corpus the probaility  of whole sentence hosted the word will valued zero (0), and the tag chain  will never be selected as possible outcome.


\subsection{Testing and Result}
HMM is pretty good classifier, but it needs to check all possible tags for a sentence $P(w_{1..n}|tag_{1..n}$ that needs a lot of execution time. So to make the algorithm efficent viterbi algorithm put to calculate the HMM. Viterbi is an algorithm belong to dynamic programming (DP) which build a very big matrix to speed up the looping process.

The constant used are $\lambda_1 = 0.1, \lambda_1 = 0.2, \lambda_1 = 0.7$ produce result with accuracy up to 94\% for tag prediction and accuracy up to 63\% for tweet prediction (whenever a tag is wrong in a tweet, the tweet count as false).


\section{Conclusion}
HMM with combination of unigram, bigram, trigram is proved as a very good classifier for Tweet POS tagging with accuracy up to 93\%. The result could be observed more by changing the multiplier constant. A lower accuracy is observed whenever the constant of unigram model is much higher than the rest.

\section{Future Work}
The project only focused on tagging process without any information learned. An information retrieval can be build as a future work of the project to learn about what is going on in the tweet. Also an sentiment analysis can be build to measure about some topic populatrity, etc.



% conference papers do not normally have an appendix



% use section* for acknowledgment
\ifCLASSOPTIONcompsoc
  % The Computer Society usually uses the plural form
  \section{Acknowledgments}
\else
  % regular IEEE prefers the singular form
  \section{Acknowledgment}
\fi


The authors would like to thank Said Al-Faraby as lecturer on NLP class and Coursera online course on NLP topic that help me to understand HMM. Also IEEE to give a latex format document that help me to write this report\cite{IEEEhowto:kopka}.





% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
\begin{thebibliography}{1}

\bibitem{POS:1}
D. Kumawat and V. Jain, “POS Tagging Approaches: A Comparison,” IJCAI , vol. 118, no. 6, pp. 32–38, May 2015.

\bibitem{POS:2}
S. Yin and G. Fan, “Research of POS Tagging Rules Mining Algorithm,” in Proceedings of the 2nd International Conference on Computer Science and Electronics Engineering (ICCSEE 2013), China, 2013.

\bibitem{POStag:PTB}
E. Atwell, \emph{The University of Pennsylvania (Penn) Treebank Tag-set}. [Online]. Available: http://www.comp.leeds.ac.uk/ccalas/tagsets/upenn.html. [Accessed: 16-Dec-2016].

\bibitem{wiki:Twitter}
Twitter. [Online]. Available: https://en.wikipedia.org/wiki/Twitter. [Accessed: 16-Dec-2016]

\bibitem{blog:Twitter}
Twitter (March 21, 2012). \emph{Twitter turns six}. Twitter.

\bibitem{HMM:1}
F. M. Hasan, N. UzZaman, and M. Khan, “Comparison of different POS Tagging Techniques (N-Gram, HMM and Brill’s tagger) for Bangla,” in Advances and Innovations in Systems, Computing Sciences and Software Engineering, Springer, 2007, pp. 121–126.

\bibitem{HMM:2}
J. Van Gael, A. Vlachos, and Z. Ghahramani, “The Infinite HMM for Unsupervised PoS Tagging,” in Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, Singapore, 2009, pp. 678–687.

\bibitem{GateTAGE}
L. Derczynski, A. Ritter, S. Clarke, and K. Bontcheva. \relax 2013. \emph{Twitter Part-of-Speech Tagging for All: Overcoming Sparse and Noisy Data}. \hskip 1em plus 0.5em minus 0.4em\relax In Proceedings of the International Conference on Recent Advances in Natural Language Processing, ACL.

\bibitem{IEEEhowto:kopka}
H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus 0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

\end{thebibliography}


% that's all folks
\end{document}


